{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3527f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q Korpora\n",
    "# !pip install transformers\n",
    "# !pip install python-mecab-kor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f0f9a161",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from Korpora import Korpora\n",
    "from mecab import MeCab\n",
    "\n",
    "from transformers import PreTrainedTokenizerFast, AutoTokenizer\n",
    "\n",
    "from custom_tokenizer import SentencePieceCustomTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bd83dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[kcbert] download kcbert-train.tar.gzaa: 100%|██████████| 2.10G/2.10G [01:46<00:00, 19.6MB/s]   \n",
      "[kcbert] download kcbert-train.tar.gzab: 100%|██████████| 2.10G/2.10G [01:00<00:00, 34.9MB/s]   \n",
      "[kcbert] download kcbert-train.tar.gzac: 671MB [00:20, 32.6MB/s]                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unzip tar. It needs a few minutes ... ./._20190101_20200611_v2.txt\n",
      "./20190101_20200611_v2.txt\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "Korpora.fetch('kcbert', root_dir='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47618a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_data = []\n",
    "with open('kcbert/20190101_20200611_v2.txt', 'r') as f:\n",
    "  for i in range(10):\n",
    "    check_data.append(f.readline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ce986c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEMO_MODE = True \n",
    "if DEMO_MODE:\n",
    "    CORPUS_SIZE = 5000000 #@param {type:\"number\"}\n",
    "    !(head -n $CORPUS_SIZE kcbert/20190101_20200611_v2.txt) > dataset.txt  \n",
    "else:\n",
    "    !mv kcbert/20190101_20200611_v2.txt dataset.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a290b32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root 693M Oct  7 07:01 dataset.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lh dataset.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3d1187a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shard_0000  shard_0004\tshard_0008  shard_0012\tshard_0016\r\n",
      "shard_0001  shard_0005\tshard_0009  shard_0013\tshard_0017\r\n",
      "shard_0002  shard_0006\tshard_0010  shard_0014\tshard_0018\r\n",
      "shard_0003  shard_0007\tshard_0011  shard_0015\tshard_0019\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir ./shards\n",
    "!split -a 4 -l 256000 -d dataset.txt ./shards/shard_\n",
    "!ls ./shards/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4bef39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset.txt', \"r\") as f:\n",
    "    data = f.read()\n",
    "    \n",
    "origin_texts = data.strip().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98099fac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(origin_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2299b785",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000000/5000000 [23:03<00:00, 3614.37it/s]\n"
     ]
    }
   ],
   "source": [
    "mecab = MeCab()\n",
    "\n",
    "with open(\"new_dataset.txt\", \"w\") as f:\n",
    "    for text in tqdm(origin_texts):\n",
    "        text = ' '.join(mecab.morphs(text))\n",
    "        f.write(text+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cc8212a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pad': {'id': 0, 'token': '[pad]'},\n",
       " 'bos': {'id': 1, 'token': '[bos]'},\n",
       " 'eos': {'id': 2, 'token': '[eos]'},\n",
       " 'unk': {'id': 3, 'token': '[unk]'}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_tokens = [\"[pad]\", \"[bos]\", \"[eos]\", \"[unk]\"]\n",
    "\n",
    "special_token_dict = {}\n",
    "\n",
    "for i in range(len(special_tokens)):\n",
    "    special_token_dict[special_tokens[i][1:-1]] = {\n",
    "        \"id\" : i,\n",
    "        \"token\" : special_tokens[i]\n",
    "    }\n",
    "\n",
    "special_token_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bb7bea54",
   "metadata": {},
   "outputs": [],
   "source": [
    "TemplateProcessing_dict = {\n",
    "    \"single\" : \"{} $A {}\".format(special_token_dict['bos']['token'], special_token_dict['eos']['token']),\n",
    "    \"pair\" : \"{} $A {} $B:1 {}:1\".format(special_token_dict['bos']['token'],special_token_dict['eos']['token'], special_token_dict['eos']['token']),\n",
    "    \"special_tokens\" : [\n",
    "        (special_token_dict['bos']['token'], special_token_dict['bos']['id']),\n",
    "        (special_token_dict['eos']['token'], special_token_dict['eos']['id']),\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7a178ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'BPE' \n",
    "# 'Unigram' \n",
    "# 'WordLevel'\n",
    "# 'WordPiece'\n",
    "\n",
    "\n",
    "norm_keys = ['Nmt', 'NFKC', 'Replace', 'Lowercase']\n",
    "\n",
    "sp_uni_tokenizer = SentencePieceCustomTokenizer('WordPiece', \n",
    "                                                norm_keys, \n",
    "                                                special_token_dict,\n",
    "                                                TemplateProcessing_dict,\n",
    "                                                add_prefix_space=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3408c72a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "CPU times: user 11min 51s, sys: 5.81 s, total: 11min 56s\n",
      "Wall time: 3min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "file = \"new_dataset.txt\"\n",
    "\n",
    "sp_uni_tokenizer.train(\n",
    "    files=[file],\n",
    "    vocab_size=30000,   # vocab size 를 지정해줄 수 있습니다.\n",
    "    show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "844389f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = PreTrainedTokenizerFast(tokenizer_object=sp_uni_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "908ff35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.add_special_tokens({'pad_token': special_token_dict['pad']['token']})\n",
    "tokenizer.bos_token_id = special_token_dict['bos']['id']\n",
    "tokenizer.cls_token_id = special_token_dict['bos']['id']\n",
    "tokenizer.eos_token_id = special_token_dict['eos']['id']\n",
    "tokenizer.sep_token_id = special_token_dict['eos']['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "21953bd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('custom_tokenizer/tokenizer_config.json',\n",
       " 'custom_tokenizer/special_tokens_map.json',\n",
       " 'custom_tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_path = 'custom_tokenizer'\n",
    "\n",
    "tokenizer.save_pretrained(tokenizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6f503e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "772c9917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 142, 21, 5991, 142, 21, 18, 5306, 9467, 21598, 9497, 2]\n",
      "[bos] 3시 30분##이 걸렸##다[eos]\n",
      "['▁', '3', '시', '▁', '3', '0', '분', '##이', '▁걸렸', '##다']\n",
      "--------------------\n",
      "[1, 142, 21, 5306, 9467, 9660, 18842, 19600, 2]\n",
      "[bos] 3분##이##서 가##세요[eos]\n",
      "['▁', '3', '분', '##이', '##서', '▁가', '##세요']\n",
      "--------------------\n",
      "[1, 19687, 9572, 24285, 9508, 19379, 9495, 9525, 9494, 31, 2]\n",
      "[bos] 오늘##의 순서##는 누구##인##가##요?[eos]\n",
      "['▁오늘', '##의', '▁순서', '##는', '▁누구', '##인', '##가', '##요', '?']\n",
      "--------------------\n",
      "[1, 26236, 9515, 10015, 9525, 9899, 9515, 9525, 18882, 10612, 9497, 16, 2]\n",
      "[bos] 앤##드##류##가##필##드##가 말##했##다.[eos]\n",
      "['▁앤', '##드', '##류', '##가', '##필', '##드', '##가', '▁말', '##했', '##다', '.']\n",
      "--------------------\n",
      "[1, 142, 21, 5991, 142, 21, 18, 5306, 2]\n",
      "[bos] 3시 30분[eos]\n",
      "['▁', '3', '시', '▁', '3', '0', '분']\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "test_texts = [\n",
    "    '3시 30분이 걸렸다',\n",
    "    '3분이서 가세요',\n",
    "    '오늘의 순서는 누구인가요?',\n",
    "    '앤드류가필드가 말했다.',\n",
    "    '3시 30분',\n",
    "]\n",
    "\n",
    "for text in test_texts: \n",
    "  print(tokenizer.encode(text))\n",
    "  print(tokenizer.decode(tokenizer.encode(text)))\n",
    "  print(tokenizer.tokenize(text))\n",
    "  print('--------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c4774a3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c3f4f533",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.bos_token_id, tokenizer.cls_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b7c2b8ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token_id, tokenizer.sep_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90315676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그외 tokenizer\n",
    "# https://github.com/huggingface/tokenizers/tree/5f6e9784526a4cd5e4f6dcdcc045cdceba5463e1/bindings/python/py_src/tokenizers/implementations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
